ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.
ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!
==((====))==  Unsloth 2025.11.2: Fast Llama patching. Transformers: 4.56.2.
   \\   /|    NVIDIA H100 NVL. Num GPUs = 2. Max memory: 93.086 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.8.0+cu128. CUDA: 9.0. CUDA Toolkit: 12.8. Triton: 3.4.0
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post2. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
{'eval_loss': 2.449594736099243, 'eval_runtime': 330.8696, 'eval_samples_per_second': 27.766, 'eval_steps_per_second': 1.738, 'epoch': 0}
Unsloth: Will smartly offload gradients to save VRAM!
{'loss': 1.4701, 'grad_norm': 0.26514366269111633, 'learning_rate': 7.965724606942175e-06, 'epoch': 1.0}
{'eval_loss': 1.3681424856185913, 'eval_runtime': 328.424, 'eval_samples_per_second': 27.973, 'eval_steps_per_second': 1.751, 'epoch': 1.0}
{'loss': 1.3584, 'grad_norm': 0.34041160345077515, 'learning_rate': 2.7463650459887536e-06, 'epoch': 2.0}
{'eval_loss': 1.3585597276687622, 'eval_runtime': 328.1062, 'eval_samples_per_second': 28.0, 'eval_steps_per_second': 1.752, 'epoch': 2.0}
{'loss': 1.3504, 'grad_norm': 0.33532169461250305, 'learning_rate': 1.148029063480216e-12, 'epoch': 3.0}
{'eval_loss': 1.3572790622711182, 'eval_runtime': 329.1712, 'eval_samples_per_second': 27.909, 'eval_steps_per_second': 1.747, 'epoch': 3.0}
{'train_runtime': 16212.6401, 'train_samples_per_second': 9.634, 'train_steps_per_second': 0.301, 'train_loss': 1.3929774232194991, 'epoch': 3.0}
 Training complete! Model checkpoints saved to ./qlora-output
